[
  {
    "objectID": "posts/probability theory/index.html",
    "href": "posts/probability theory/index.html",
    "title": "Probability Theory and Random Variables",
    "section": "",
    "text": "Bayes’ rule, or Bayes’ theorem or Bayes’ law, describes the probability of an event occurring given prior knowledge of conditions that could be related to said event.\nI am interested in exploring the relationship between a country’s happiness score and how much renewable energy the country uses. Happiness scores have been measured for citizens of countries using the World Happiness Report, which is gathered from the Gallup World Poll. The happiness score is also known as the Cantril score because the main question asked to determine a citizens happiness is: “Please imagine a ladder, with steps numbered from 0 at the bottom to 10 at the top. The top of the ladder represents the best possible life for you and the bottom of the ladder represents the worst possible life for you. On which step of the ladder would you say you personally feel you stand at this time?” And the ladder in reference is also called the Cantril ladder.\nMy data resources are https://ourworldindata.org/renewable-energy for renewable energy and https://ourworldindata.org/happiness-and-life-satisfaction for happiness.\nFirst, import all libraries and pull data from github into pandas dataframes.\n\n\nCode\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import r2_score\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.metrics import accuracy_score\n\nhappy_csv = 'https://raw.githubusercontent.com/erica16/ml1_blogs/main/data/happiness-cantril-ladder.csv'\nrenew_csv = 'https://raw.githubusercontent.com/erica16/ml1_blogs/main/data/renewable-share-energy.csv'\npercent_renew_csv = 'https://raw.githubusercontent.com/erica16/ml1_blogs/main/data/modern-renewable-energy-consumption.csv'\n\n# read in datasets\nhappy_df = pd.read_csv(happy_csv)\nrenew_df = pd.read_csv(renew_csv)\npercent_renew_df = pd.read_csv(percent_renew_csv)\n\n\nFor this task, I have 3 datasets - one for the happiness metric, one for the percentage of country’s energy that comes from renewable sources, and one for the breakdown of a country’s renewable energy sources in TWh (terawatt hours). Since the amount of energy a country consumes varies per country, we need to have a way of being able to compare the renewable energy source breakdowns reliably between countries. For this, I made a function that takes all the renewable sources and turns each value into a percentage.\n\n\nCode\n# takes row in the renewable energy consumption breakdown datatable and turns each data column\n# into a percent value rounded to the nearest 100ths decimal place.\ndef calculate_percentages(row):\n    other = row[3]\n    solar = row[4]\n    wind = row[5]\n    hydro = row[6]\n    total = other + wind + solar + hydro\n    if (total == 0):\n        return pd.Series([0, 0, 0, 0])\n    else:\n        return pd.Series(\n            [(other/total) * 100, \n            (solar/total) * 100, \n            (wind/total) * 100, \n            (hydro/total) * 100\n            ])\n\n\nNow that we have a good way of comparing all the data on the same scale, we can get our dataframes ready to merge and then analyze. First we can narrow down the dataframes to just the columns we need, and filter the data to just years 2011 and after because the happiness index dataset starts at 2011.\n\n\nCode\n# clean data\npercent_renew_df[['Other', 'Solar', 'Wind', 'Hydro']] = percent_renew_df.apply(calculate_percentages, axis=1)\npercent_renew_clean = percent_renew_df.filter(['Entity', 'Year', 'Other', 'Solar', 'Wind', 'Hydro'])\npercent_renew_clean['Largest Renewable Source'] = percent_renew_clean[['Other', 'Solar', 'Wind', 'Hydro']].idxmax(axis=1)\nhappy_df_rename = happy_df.rename(columns={'Cantril ladder score':'Happiness Score'})\nrenew_df_rename = renew_df.rename(columns={renew_df.columns[3]:'Percent Renewable Energy'})\n\n# filter to after 2010 because the happiness dataset starts at 2011\nhappy_years = happy_df_rename.loc[happy_df_rename['Year'] &gt; 2010]\nrenew_years = renew_df_rename.loc[renew_df_rename['Year']&gt; 2010]\npercent_renew_years = percent_renew_clean.loc[percent_renew_clean['Year'] &gt; 2010]\n\n# merge dfs on country\nhappy_renew_df = happy_years.merge(renew_years, on='Entity')\n\nall_df = happy_renew_df.merge(percent_renew_years, on='Entity')\n\nall_xy = all_df.filter(['Happiness Score', 'Percent Renewable Energy', 'Solar', 'Wind', 'Hydro', 'Other', 'Largest Renewable Source']).dropna()\n\nprint(all_xy.info())\nsns.scatterplot(data=all_xy, x='Percent Renewable Energy', y='Happiness Score', s=12, hue='Largest Renewable Source')\nplt.title(\"Country Happiness Score vs Percent Renewable Energy\")\nplt.show()\n\n\nC:\\Users\\Erica\\AppData\\Local\\Temp\\ipykernel_74056\\3612890870.py:4: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n  other = row[3]\nC:\\Users\\Erica\\AppData\\Local\\Temp\\ipykernel_74056\\3612890870.py:5: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n  solar = row[4]\nC:\\Users\\Erica\\AppData\\Local\\Temp\\ipykernel_74056\\3612890870.py:6: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n  wind = row[5]\nC:\\Users\\Erica\\AppData\\Local\\Temp\\ipykernel_74056\\3612890870.py:7: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n  hydro = row[6]\nC:\\Users\\Erica\\AppData\\Local\\Temp\\ipykernel_74056\\1829548257.py:4: FutureWarning: The behavior of DataFrame.idxmax with all-NA values, or any-NA and skipna=False, is deprecated. In a future version this will raise ValueError\n  percent_renew_clean['Largest Renewable Source'] = percent_renew_clean[['Other', 'Solar', 'Wind', 'Hydro']].idxmax(axis=1)\n\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nIndex: 97440 entries, 0 to 97559\nData columns (total 7 columns):\n #   Column                    Non-Null Count  Dtype  \n---  ------                    --------------  -----  \n 0   Happiness Score           97440 non-null  float64\n 1   Percent Renewable Energy  97440 non-null  float64\n 2   Solar                     97440 non-null  float64\n 3   Wind                      97440 non-null  float64\n 4   Hydro                     97440 non-null  float64\n 5   Other                     97440 non-null  float64\n 6   Largest Renewable Source  97440 non-null  object \ndtypes: float64(6), object(1)\nmemory usage: 5.9+ MB\nNone\n\n\n\n\n\nHere is our first look at the dataset. From the scatterplot, we can see that there is clearly not a tight relationship between the variables, but it does seem to be generally a positive trend.\nNow we can run some models on the data and try to see a relationship, starting with a linear regression. Before running the linear regression, my guess is what I stated in the previous paragraph - there will be a loose positive trend.\n\n\nCode\nX = all_xy['Percent Renewable Energy']\ny = all_xy['Happiness Score']\n\n# linear regression\ntrain_set_X, test_set_X, train_set_y, test_set_y= train_test_split(np.array(X).reshape(-1, 1), np.array(y), test_size=0.2, random_state=42)\n\nlin_reg = LinearRegression()\nlin_reg.fit(train_set_X, train_set_y)\nlin_reg.intercept_, lin_reg.coef_\ny_pred = lin_reg.predict(test_set_X)\n\nsns.scatterplot(data=all_xy, x='Percent Renewable Energy', y='Happiness Score', s=12, hue='Largest Renewable Source')\nplt.plot(test_set_X, y_pred, color='red')\nplt.title(\"Country Happiness Score vs Percent Renewable Energy\")\nplt.show()\nprint(\"r-squared = {:.3f}\".format(r2_score(X, y)))\n\n\n\n\n\nr-squared = -0.334\n\n\nNow that we can see the positive trendline, we know my guess was right. That r-squared value is pretty unfortunate though. A negative r-squared value means the model performs very poorly for this dataset, so it may be useful to check out a different type of model.\nInstead of doing a Linear Regression, I am doing a Gaussian Naive Bayes model. This is a model that assumes that the numerical attributes like Happiness Score are distributed normally, and assumes independence among the features in order to apply Bayes’ theorem.\nSince it needs to compare a categorical variable with a continuous numerical variable, I am going to be using the Happiness Score to predict a country’s Largest Renewable Resource.\n\n\nCode\nX = all_xy['Happiness Score']\ny = all_xy['Largest Renewable Source']\ntrain_set_X, test_set_X, train_set_y, test_set_y= train_test_split(np.array(X).reshape(-1, 1), np.array(y), test_size=0.2, random_state=42)\n\ngnb = GaussianNB()\nprint(train_set_X)\nprint(train_set_y)\ngnb.fit(train_set_X, train_set_y)\ngnb_predict = gnb.predict(test_set_X)\n\nprint(accuracy_score(test_set_y, gnb_predict))\n\n\n[[6.3338  ]\n [5.234   ]\n [5.089082]\n ...\n [4.8141  ]\n [5.2946  ]\n [6.1718  ]]\n['Hydro' 'Hydro' 'Hydro' ... 'Wind' 'Solar' 'Hydro']\n0.6616892446633826\n\n\nThe Gaussian Naive Bayes model is 66% accurate! This means that 66 times out of 100, the model will correctly predict a country’s largest renewable source given the country’s happiness score."
  },
  {
    "objectID": "posts/linear-regression/index.html",
    "href": "posts/linear-regression/index.html",
    "title": "Linear Regression",
    "section": "",
    "text": "A Linear regression is a way of displaying a predictive relationship between an independent variable and a dependent variable. It allows data scientists to calculate a line of best fit through the points displayed on a scatterplot, which can then be used to describe the relationship of the independent and dependent variable, in addition to allowing for prediction of a dependent variable value given an independent variable value.\n\n\n\nI am going to examine the relationship between months of job experience and salary, with job experience being the independent variable and salary being the dependent variable. My hypothesis is that as months of job experience increase, salary will also increase. My data source: https://www.kaggle.com/datasets/saquib7hussain/experience-salary-dataset\n\n\n\n\n\nCode\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import r2_score\nfrom sklearn.linear_model import LinearRegression\n\nexp_csv = pd.read_csv('https://raw.githubusercontent.com/erica16/ml1_blogs/main/data/Experience-Salary.csv')\n\n\n\n\n\nSince I want to study the effect of experience on salary, X (the independent variable) will be experience in months, and y (the dependent variable) will be salary in thousands of dollars. Because I’m doing a linear regression, the first thing I do after taking a look at the information about the data is split the data up into training and testing sets using the scikit learn train_test_split method. After that, I fit the linear regression to the training data then predict y using the test set of x values. From there the only thing left to do is visualize the data using a scatterplot to show the whole dataset and a line to show the linear regression using the test x and predicted y.\n\n\nCode\n# linear regression\nX = exp_csv['exp(in months)']\ny = exp_csv['salary(in thousands)']\n\nprint(exp_csv.info())\n\ntrain_set_X, test_set_X, train_set_y, test_set_y= train_test_split(np.array(X).reshape(-1, 1), y, test_size=0.2, random_state=42)\n\nlin_reg = LinearRegression()\nlin_reg.fit(train_set_X, train_set_y)\ny_pred = lin_reg.predict(test_set_X)\n\nsns.scatterplot(x=X, y=y)\nplt.plot(test_set_X, y_pred, color='red')\nplt.title('Salary in thousands vs Job Experience in months')\nplt.show()\n\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 1000 entries, 0 to 999\nData columns (total 2 columns):\n #   Column                Non-Null Count  Dtype  \n---  ------                --------------  -----  \n 0   exp(in months)        1000 non-null   float64\n 1   salary(in thousands)  1000 non-null   float64\ndtypes: float64(2)\nmemory usage: 15.8 KB\nNone\n\n\n\n\n\n\n\n\nThat line looks pretty well fit! To find out how well fit, we can take a look at the R-squared value:\n\n\nCode\nprint(\"r-squared = {:.3f}\".format(r2_score(X, y)))\n\n\nr-squared = 0.612\n\n\nR-squared values range from 0 to 1, and generally in social science research a value between 0.50 and 0.99 is acceptable, according to this link. Therefore, this is a fairly good model! It could always be closer to 1, but that is for another blog."
  },
  {
    "objectID": "posts/linear-regression/index.html#what-is-linear-regression",
    "href": "posts/linear-regression/index.html#what-is-linear-regression",
    "title": "Linear Regression",
    "section": "",
    "text": "A Linear regression is a way of displaying a predictive relationship between an independent variable and a dependent variable. It allows data scientists to calculate a line of best fit through the points displayed on a scatterplot, which can then be used to describe the relationship of the independent and dependent variable, in addition to allowing for prediction of a dependent variable value given an independent variable value."
  },
  {
    "objectID": "posts/linear-regression/index.html#about-the-data",
    "href": "posts/linear-regression/index.html#about-the-data",
    "title": "Linear Regression",
    "section": "",
    "text": "I am going to examine the relationship between months of job experience and salary, with job experience being the independent variable and salary being the dependent variable. My hypothesis is that as months of job experience increase, salary will also increase. My data source: https://www.kaggle.com/datasets/saquib7hussain/experience-salary-dataset"
  },
  {
    "objectID": "posts/linear-regression/index.html#step-1-import-libraries-and-read-in-dataset-to-a-pandas-dataframe",
    "href": "posts/linear-regression/index.html#step-1-import-libraries-and-read-in-dataset-to-a-pandas-dataframe",
    "title": "Linear Regression",
    "section": "",
    "text": "Code\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import r2_score\nfrom sklearn.linear_model import LinearRegression\n\nexp_csv = pd.read_csv('https://raw.githubusercontent.com/erica16/ml1_blogs/main/data/Experience-Salary.csv')"
  },
  {
    "objectID": "posts/linear-regression/index.html#step-2-perform-linear-regression",
    "href": "posts/linear-regression/index.html#step-2-perform-linear-regression",
    "title": "Linear Regression",
    "section": "",
    "text": "Since I want to study the effect of experience on salary, X (the independent variable) will be experience in months, and y (the dependent variable) will be salary in thousands of dollars. Because I’m doing a linear regression, the first thing I do after taking a look at the information about the data is split the data up into training and testing sets using the scikit learn train_test_split method. After that, I fit the linear regression to the training data then predict y using the test set of x values. From there the only thing left to do is visualize the data using a scatterplot to show the whole dataset and a line to show the linear regression using the test x and predicted y.\n\n\nCode\n# linear regression\nX = exp_csv['exp(in months)']\ny = exp_csv['salary(in thousands)']\n\nprint(exp_csv.info())\n\ntrain_set_X, test_set_X, train_set_y, test_set_y= train_test_split(np.array(X).reshape(-1, 1), y, test_size=0.2, random_state=42)\n\nlin_reg = LinearRegression()\nlin_reg.fit(train_set_X, train_set_y)\ny_pred = lin_reg.predict(test_set_X)\n\nsns.scatterplot(x=X, y=y)\nplt.plot(test_set_X, y_pred, color='red')\nplt.title('Salary in thousands vs Job Experience in months')\nplt.show()\n\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 1000 entries, 0 to 999\nData columns (total 2 columns):\n #   Column                Non-Null Count  Dtype  \n---  ------                --------------  -----  \n 0   exp(in months)        1000 non-null   float64\n 1   salary(in thousands)  1000 non-null   float64\ndtypes: float64(2)\nmemory usage: 15.8 KB\nNone"
  },
  {
    "objectID": "posts/linear-regression/index.html#step-3-check-out-the-r-squared-value",
    "href": "posts/linear-regression/index.html#step-3-check-out-the-r-squared-value",
    "title": "Linear Regression",
    "section": "",
    "text": "That line looks pretty well fit! To find out how well fit, we can take a look at the R-squared value:\n\n\nCode\nprint(\"r-squared = {:.3f}\".format(r2_score(X, y)))\n\n\nr-squared = 0.612\n\n\nR-squared values range from 0 to 1, and generally in social science research a value between 0.50 and 0.99 is acceptable, according to this link. Therefore, this is a fairly good model! It could always be closer to 1, but that is for another blog."
  },
  {
    "objectID": "posts/classification/index.html",
    "href": "posts/classification/index.html",
    "title": "Random Forest Classifier",
    "section": "",
    "text": "In college we are told that if we spend a lot of time studying, make sure to listen well in class, make sure to GO to class, and take good notes we will succeed. But does this advice hold up to scrutiny? We are about to find out. Using a dataset that has a wide range of metrics on undergraduate college students, I will use a random forest classifier to determine what features are most important to a student’s grade.\ndatset: https://www.kaggle.com/datasets/jacksondivakarr/student-classification-dataset/data\n\n\n\n\nCode\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.compose import make_column_selector\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.model_selection import train_test_split\n\n# get data\nstudent_df = pd.read_csv('https://raw.githubusercontent.com/erica16/ml1_blogs/main/data/student.csv')\n\nprint(student_df.head())\n\n\n   Unnamed: 0    Id  Student_Age     Sex High_School_Type Scholarship  \\\n0           0  5001           21    Male            Other         50%   \n1           1  5002           20    Male            Other         50%   \n2           2  5003           21    Male            State         50%   \n3           3  5004           18  Female          Private         50%   \n4           4  5005           22    Male          Private         50%   \n\n  Additional_Work Sports_activity Transportation  Weekly_Study_Hours  \\\n0             Yes              No        Private                   0   \n1             Yes              No        Private                   0   \n2              No              No        Private                   2   \n3             Yes              No            Bus                   2   \n4              No              No            Bus                  12   \n\n  Attendance Reading Notes Listening_in_Class Project_work Grade  \n0     Always     Yes   Yes                 No           No    AA  \n1     Always     Yes    No                Yes          Yes    AA  \n2      Never      No    No                 No          Yes    AA  \n3     Always      No   Yes                 No           No    AA  \n4     Always     Yes    No                Yes          Yes    AA  \n\n\n\n\n\nSince a lot of this data is categorical, we will need to use an encoder to make it numerical so the random forest classifier can handle it. For this task I am using a label encoder because the majority of the categorical data is not ordered and has a small set of repeated values.\n\n\nCode\n#dropping ID because it will not be used to train or be predicted\nstudent_df_clean = student_df.drop(student_df.columns[:2], axis='columns')\ncat_selector = make_column_selector(dtype_include=object)\ncat_columns = cat_selector(student_df_clean)\n\n# use encoder for categorical data\nstudent_df_enc = student_df_clean.select_dtypes(include=['number'])\nlabel_encoder = LabelEncoder()\n\n# Iterate through columns and encode categorical columns\nfor col in student_df_clean.columns:\n    if student_df_clean[col].dtype == 'object':  # Check if column has categorical data\n        student_df_enc[col] = label_encoder.fit_transform(student_df_clean[col])\n\nprint(student_df_enc.head())\n\n\n   Student_Age  Weekly_Study_Hours  Sex  High_School_Type  Scholarship  \\\n0           21                   0    1                 0            2   \n1           20                   0    1                 0            2   \n2           21                   2    1                 2            2   \n3           18                   2    0                 1            2   \n4           22                  12    1                 1            2   \n\n   Additional_Work  Sports_activity  Transportation  Attendance  Reading  \\\n0                1                0               1           1        1   \n1                1                0               1           1        1   \n2                0                0               1           2        0   \n3                1                0               0           1        0   \n4                0                0               0           1        1   \n\n   Notes  Listening_in_Class  Project_work  Grade  \n0      2                   1             0      0  \n1      1                   2             1      0  \n2      1                   1             1      0  \n3      2                   1             0      0  \n4      1                   2             1      0  \n\n\n\n\n\nNow that the data is clean we can split it up into training and testing data using the scikit learn train_test_split function. Since we want to know what factors impact grade, the independent variables (X) will be all the factors besides grade, and the dependent variable (y) will be grade.\n\n\nCode\ntrain_set_X, test_set_X, train_set_y, test_set_y = train_test_split(student_df_enc.drop('Grade', axis='columns'), student_df_enc['Grade'], test_size=0.2, random_state=42)\n\nrnd_clf = RandomForestClassifier(n_estimators=500, max_leaf_nodes=16,\n                                 n_jobs=-1, random_state=42)\nrnd_clf.fit(train_set_X, train_set_y)\n\ny_pred_rf = rnd_clf.predict(test_set_X)\n\n# take a look at feature importances to see what to use as the independent value\n\nfeature_importances = pd.Series(rnd_clf.feature_importances_, train_set_X.columns).sort_values()\n\nplt.barh(feature_importances.index, feature_importances.values * 100)\nplt.xlabel('Percent Impact on Grade')\nplt.ylabel('Student Factor')\nplt.title('Impact on Student Activities on Grade')\nplt.show()\n\nprint(f\"Accuracy score: {accuracy_score(test_set_y, y_pred_rf) * 100}%\")\n\n\n\n\n\nAccuracy score: 20.689655172413794%\n\n\n\n\n\nUpon viewing the top factors that impact a student’s grade, I am compelled to remind the reader that correlation does not mean causation. Surely there are other forces at play that make a student’s age the number one most impactful factor on their grade, and whether they do or don’t have a scholarship also probably isn’t the cause of good grades. It is also important to keep an eye on the scale of this chart - the highest factor is only about 14% impactful, and the highest factor that a student can actually control is about 9% impactful. It doesn’t look like any one thing will have a huge impact on grades, which makes sense. If a student starts going to class more, for example, but doesn’t pay attention or take notes, that most likely won’t cause a major impact in their grade (unless their grade is 100% attendance based, but that is also unlikely). The other thing to point out is the accuracy score - this random forest classifier is only 20% accurate. That means there is a lot of room for improvement! Perhaps more data, or better quality data, or more tweaking the label encoder could increase this number."
  },
  {
    "objectID": "posts/classification/index.html#step-1-import-libraries-and-dataset",
    "href": "posts/classification/index.html#step-1-import-libraries-and-dataset",
    "title": "Random Forest Classifier",
    "section": "",
    "text": "Code\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.compose import make_column_selector\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.model_selection import train_test_split\n\n# get data\nstudent_df = pd.read_csv('https://raw.githubusercontent.com/erica16/ml1_blogs/main/data/student.csv')\n\nprint(student_df.head())\n\n\n   Unnamed: 0    Id  Student_Age     Sex High_School_Type Scholarship  \\\n0           0  5001           21    Male            Other         50%   \n1           1  5002           20    Male            Other         50%   \n2           2  5003           21    Male            State         50%   \n3           3  5004           18  Female          Private         50%   \n4           4  5005           22    Male          Private         50%   \n\n  Additional_Work Sports_activity Transportation  Weekly_Study_Hours  \\\n0             Yes              No        Private                   0   \n1             Yes              No        Private                   0   \n2              No              No        Private                   2   \n3             Yes              No            Bus                   2   \n4              No              No            Bus                  12   \n\n  Attendance Reading Notes Listening_in_Class Project_work Grade  \n0     Always     Yes   Yes                 No           No    AA  \n1     Always     Yes    No                Yes          Yes    AA  \n2      Never      No    No                 No          Yes    AA  \n3     Always      No   Yes                 No           No    AA  \n4     Always     Yes    No                Yes          Yes    AA"
  },
  {
    "objectID": "posts/classification/index.html#step-2-clean-data",
    "href": "posts/classification/index.html#step-2-clean-data",
    "title": "Random Forest Classifier",
    "section": "",
    "text": "Since a lot of this data is categorical, we will need to use an encoder to make it numerical so the random forest classifier can handle it. For this task I am using a label encoder because the majority of the categorical data is not ordered and has a small set of repeated values.\n\n\nCode\n#dropping ID because it will not be used to train or be predicted\nstudent_df_clean = student_df.drop(student_df.columns[:2], axis='columns')\ncat_selector = make_column_selector(dtype_include=object)\ncat_columns = cat_selector(student_df_clean)\n\n# use encoder for categorical data\nstudent_df_enc = student_df_clean.select_dtypes(include=['number'])\nlabel_encoder = LabelEncoder()\n\n# Iterate through columns and encode categorical columns\nfor col in student_df_clean.columns:\n    if student_df_clean[col].dtype == 'object':  # Check if column has categorical data\n        student_df_enc[col] = label_encoder.fit_transform(student_df_clean[col])\n\nprint(student_df_enc.head())\n\n\n   Student_Age  Weekly_Study_Hours  Sex  High_School_Type  Scholarship  \\\n0           21                   0    1                 0            2   \n1           20                   0    1                 0            2   \n2           21                   2    1                 2            2   \n3           18                   2    0                 1            2   \n4           22                  12    1                 1            2   \n\n   Additional_Work  Sports_activity  Transportation  Attendance  Reading  \\\n0                1                0               1           1        1   \n1                1                0               1           1        1   \n2                0                0               1           2        0   \n3                1                0               0           1        0   \n4                0                0               0           1        1   \n\n   Notes  Listening_in_Class  Project_work  Grade  \n0      2                   1             0      0  \n1      1                   2             1      0  \n2      1                   1             1      0  \n3      2                   1             0      0  \n4      1                   2             1      0"
  },
  {
    "objectID": "posts/classification/index.html#step-3-train-and-run-random-forest-classifier",
    "href": "posts/classification/index.html#step-3-train-and-run-random-forest-classifier",
    "title": "Random Forest Classifier",
    "section": "",
    "text": "Now that the data is clean we can split it up into training and testing data using the scikit learn train_test_split function. Since we want to know what factors impact grade, the independent variables (X) will be all the factors besides grade, and the dependent variable (y) will be grade.\n\n\nCode\ntrain_set_X, test_set_X, train_set_y, test_set_y = train_test_split(student_df_enc.drop('Grade', axis='columns'), student_df_enc['Grade'], test_size=0.2, random_state=42)\n\nrnd_clf = RandomForestClassifier(n_estimators=500, max_leaf_nodes=16,\n                                 n_jobs=-1, random_state=42)\nrnd_clf.fit(train_set_X, train_set_y)\n\ny_pred_rf = rnd_clf.predict(test_set_X)\n\n# take a look at feature importances to see what to use as the independent value\n\nfeature_importances = pd.Series(rnd_clf.feature_importances_, train_set_X.columns).sort_values()\n\nplt.barh(feature_importances.index, feature_importances.values * 100)\nplt.xlabel('Percent Impact on Grade')\nplt.ylabel('Student Factor')\nplt.title('Impact on Student Activities on Grade')\nplt.show()\n\nprint(f\"Accuracy score: {accuracy_score(test_set_y, y_pred_rf) * 100}%\")\n\n\n\n\n\nAccuracy score: 20.689655172413794%"
  },
  {
    "objectID": "posts/classification/index.html#investigate-results",
    "href": "posts/classification/index.html#investigate-results",
    "title": "Random Forest Classifier",
    "section": "",
    "text": "Upon viewing the top factors that impact a student’s grade, I am compelled to remind the reader that correlation does not mean causation. Surely there are other forces at play that make a student’s age the number one most impactful factor on their grade, and whether they do or don’t have a scholarship also probably isn’t the cause of good grades. It is also important to keep an eye on the scale of this chart - the highest factor is only about 14% impactful, and the highest factor that a student can actually control is about 9% impactful. It doesn’t look like any one thing will have a huge impact on grades, which makes sense. If a student starts going to class more, for example, but doesn’t pay attention or take notes, that most likely won’t cause a major impact in their grade (unless their grade is 100% attendance based, but that is also unlikely). The other thing to point out is the accuracy score - this random forest classifier is only 20% accurate. That means there is a lot of room for improvement! Perhaps more data, or better quality data, or more tweaking the label encoder could increase this number."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this blog"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Machine Learning",
    "section": "",
    "text": "Random Forest Classifier\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n\n  \n\n\n\n\nClustering\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n\n  \n\n\n\n\nLinear Regression\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n\n  \n\n\n\n\nOutlier Detection\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n\n  \n\n\n\n\nProbability Theory and Random Variables\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/clustering/index.html",
    "href": "posts/clustering/index.html",
    "title": "Clustering",
    "section": "",
    "text": "As an avid coffee drinker, I have always been impressed by people who can give a cup of black coffee a good sniff and tell you exactly where it was grown or the bean variety. I want to know if a machine learning algorithm could “smell out” coffee bean variety based on several factors. Data set source: https://www.kaggle.com/datasets/fatihb/coffee-quality-data-cqi\n\n\n\n\nCode\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.metrics import r2_score\nfrom sklearn.cluster import KMeans\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.ensemble import RandomForestClassifier\nimport re\n\ncoffee_csv = 'https://raw.githubusercontent.com/erica16/ml1_blogs/main/data/df_arabica_clean.csv'\n\ncoffee_df = pd.read_csv(coffee_csv)\n\n\n\n\n\nBefore I do anything I need to know what type of data I’m dealing with here.\n\n\nCode\ncoffee_df.info()\n\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 207 entries, 0 to 206\nData columns (total 41 columns):\n #   Column                 Non-Null Count  Dtype  \n---  ------                 --------------  -----  \n 0   Unnamed: 0             207 non-null    int64  \n 1   ID                     207 non-null    int64  \n 2   Country of Origin      207 non-null    object \n 3   Farm Name              205 non-null    object \n 4   Lot Number             206 non-null    object \n 5   Mill                   204 non-null    object \n 6   ICO Number             75 non-null     object \n 7   Company                207 non-null    object \n 8   Altitude               206 non-null    object \n 9   Region                 205 non-null    object \n 10  Producer               206 non-null    object \n 11  Number of Bags         207 non-null    int64  \n 12  Bag Weight             207 non-null    object \n 13  In-Country Partner     207 non-null    object \n 14  Harvest Year           207 non-null    object \n 15  Grading Date           207 non-null    object \n 16  Owner                  207 non-null    object \n 17  Variety                201 non-null    object \n 18  Status                 207 non-null    object \n 19  Processing Method      202 non-null    object \n 20  Aroma                  207 non-null    float64\n 21  Flavor                 207 non-null    float64\n 22  Aftertaste             207 non-null    float64\n 23  Acidity                207 non-null    float64\n 24  Body                   207 non-null    float64\n 25  Balance                207 non-null    float64\n 26  Uniformity             207 non-null    float64\n 27  Clean Cup              207 non-null    float64\n 28  Sweetness              207 non-null    float64\n 29  Overall                207 non-null    float64\n 30  Defects                207 non-null    float64\n 31  Total Cup Points       207 non-null    float64\n 32  Moisture Percentage    207 non-null    float64\n 33  Category One Defects   207 non-null    int64  \n 34  Quakers                207 non-null    int64  \n 35  Color                  207 non-null    object \n 36  Category Two Defects   207 non-null    int64  \n 37  Expiration             207 non-null    object \n 38  Certification Body     207 non-null    object \n 39  Certification Address  207 non-null    object \n 40  Certification Contact  207 non-null    object \ndtypes: float64(13), int64(6), object(22)\nmemory usage: 66.4+ KB\n\n\n\n\n\nA lot of these columns are not related to the coffee bean itself, so I’ll want to remove those. It also appears that I will need to handle null values, even after I take out the ICO number. I’ll go ahead and pare the dataset down to just the columns that are related to the coffee beans, but before I can remove null values I need to get the Altitude values from a string type to an int type so that I can use them more accurately. I wrote a function that uses regex to pull out the int values from the Altitude string into an array of stringified ints, which I turn into ints and then average so I only have one value for each Altitude. After that, I need the categorical columns to be useable by the machine learning algorithms, so I use a label encoder to turn the categorical data into numerical data. This dataset also isn’t very big, so I use an imputer to change null values to plausible values instead of dropping rows with null values.\n\n\nCode\n# clean data\ncoffee_df_small = coffee_df[['Country of Origin', 'Altitude', 'Region', 'Variety', 'Processing Method', 'Aroma', 'Flavor']].copy()\n\n# altitude has some values that are ranges, which is not ideal because it makes it a string not an int, so we'll take the average and replace the value with that\ndef get_altitude_int(alt_in):\n  ints = re.findall(r'\\b\\d+\\b', alt_in)\n  int_values = [int(num) for num in ints]\n  return int(np.mean(int_values))\n\naltitude_no_na = coffee_df_small['Altitude'].notnull()\ncoffee_df_small.loc[altitude_no_na, 'Altitude Avg'] = coffee_df_small.loc[altitude_no_na, 'Altitude'].apply(get_altitude_int)\n\n# turn categorical columns into numerical\ncoffee_df_nums = coffee_df_small.select_dtypes(include=['number'])\nlabel_encoder = LabelEncoder()\n\nfor col in coffee_df_small.columns:\n    if coffee_df_small[col].dtype == 'object':  # Check if column has categorical data\n        coffee_df_nums[col] = label_encoder.fit_transform(coffee_df_small[col])\n\nimputer = SimpleImputer(strategy=\"median\")\nimputer.fit(coffee_df_nums)\ncoffee_imp = pd.DataFrame(imputer.transform(coffee_df_nums), columns=coffee_df_nums.columns)\nprint(coffee_imp.info())\n\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 207 entries, 0 to 206\nData columns (total 8 columns):\n #   Column             Non-Null Count  Dtype  \n---  ------             --------------  -----  \n 0   Aroma              207 non-null    float64\n 1   Flavor             207 non-null    float64\n 2   Altitude Avg       207 non-null    float64\n 3   Country of Origin  207 non-null    float64\n 4   Altitude           207 non-null    float64\n 5   Region             207 non-null    float64\n 6   Variety            207 non-null    float64\n 7   Processing Method  207 non-null    float64\ndtypes: float64(8)\nmemory usage: 13.1 KB\nNone\n\n\n\n\n\nI am going to use a random forest classifier to determine what features are most relevant to predicting coffee variety.\n\n\nCode\n# do random forest to find out what the most important features are\ncoffee_enc_copy = coffee_imp.copy()\ntrain_set_X, test_set_X, train_set_y, test_set_y = train_test_split(coffee_enc_copy.drop('Variety', axis='columns'), coffee_enc_copy['Variety'], test_size=0.2, random_state=42)\n\nrnd_clf_coffee = RandomForestClassifier(n_estimators=500, max_leaf_nodes=16,\n                                 n_jobs=-1, random_state=42)\nrnd_clf_coffee.fit(train_set_X, train_set_y)\n\n# look at the relationship\nfeature_importances = pd.Series(rnd_clf_coffee.feature_importances_, train_set_X.columns).sort_values()\n\nplt.barh(feature_importances.index, feature_importances.values * 100, color='brown')\nplt.ylabel('Feature')\nplt.xlabel('Percentage Feature Importance')\nplt.title('Feature Importance of Coffee Features to Coffee Variety')\nplt.show()\n\n\n\n\n\nThere are a couple interesting things to note in this bar chart. The first is the order of the important features: altitude average is most important at a little over 20%, then Country of Origin at around 20%. The second is that altitude average is more important than altitude, but I created altitude average from altitude, so shouldn’t they be the same? The reason they are not is that the label encoder assumes there is no relation between the unique values in the column, so a super high altitude could be labeled 0 and a super low altitude could be labeled 207 (since that is the number of items in each column). Clearly, being able to understand that altitude is a numerical value is important to this model. With that in mind, that also means that country of origin might be less useable after being run through a label encoder than if I had found a way of imparting relative geographic location into the numerical representation of country of origin.\n\n\n\nThe top 2 features are altitude average and country of origin, so naturally we will do our K-Means clustering algorithm on those variables. I want altitude on the y axis because it is the vertical column and is a good representation of actual altitude. I will also use 7 clusters because that is how many varieties of coffee there are in this dataset.\n\n\nCode\n# aroma_alt_np = np.array(coffee_imp[['Aroma', 'Altitude Avg']])\naltitude_country_np = np.array(coffee_imp[['Country of Origin', 'Altitude Avg']])\n# k-means clusterer\nk = 7\nkmeans = KMeans(n_clusters=k, random_state=42)\ny_pred = kmeans.fit_predict(altitude_country_np)\n\n# plot decision boundaries\nmins = altitude_country_np.min(axis=0) - 0.1\nmaxs = altitude_country_np.max(axis=0) + 0.1\nxx, yy = np.meshgrid(np.linspace(mins[0], maxs[0], 1000),\n                      np.linspace(mins[1], maxs[1], 1000))\nZ = kmeans.predict(np.c_[xx.ravel(), yy.ravel()])\nZ = Z.reshape(xx.shape)\n\nplt.contourf(Z, extent=(mins[0], maxs[0], mins[1], maxs[1]),\n            cmap=\"copper\", alpha=0.6)\nplt.contour(Z, extent=(mins[0], maxs[0], mins[1], maxs[1]),\n            linewidths=1, colors='k')\n\n# plot data\nsns.scatterplot(data=coffee_imp, x='Country of Origin', y='Altitude Avg', s=20, hue='Variety', palette=sns.color_palette(\"Set2\", as_cmap=True))\n\n# plot centroids\ncentroids = kmeans.cluster_centers_\nplt.scatter(centroids[:, 0], centroids[:, 1],\n                marker='o', s=35, linewidths=8,\n                color='white', zorder=10, alpha=0.9)\nplt.scatter(centroids[:, 0], centroids[:, 1],\n                marker='x', s=2, linewidths=12,\n                color='black', zorder=11, alpha=1)\n\n\nC:\\Users\\Erica\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n  super()._check_params_vs_input(X, default_n_init=10)\n\n\n&lt;matplotlib.collections.PathCollection at 0x1fb83bf7740&gt;\n\n\n\n\n\n\n\n\nThat chart looks… not great. One reason could be because Country of Origin was originally categorical, and the label encoder may have not done a great job. Just for fun, we’ll take the feature with the highest feature importance to variety that started out as numerical - aroma. That is how people can tell between coffee beans, lets hope the same goes for a K-Means clustering algorithm!\n\n\nCode\naroma_alt_np = np.array(coffee_imp[['Aroma', 'Altitude Avg']])\n# k-means clusterer\nk = 7\nkmeans = KMeans(n_clusters=k, random_state=42)\ny_pred = kmeans.fit_predict(aroma_alt_np)\n\n# plot decision boundaries\nmins = aroma_alt_np.min(axis=0) - 0.1\nmaxs = aroma_alt_np.max(axis=0) + 0.1\nxx, yy = np.meshgrid(np.linspace(mins[0], maxs[0], 1000),\n                      np.linspace(mins[1], maxs[1], 1000))\nZ = kmeans.predict(np.c_[xx.ravel(), yy.ravel()])\nZ = Z.reshape(xx.shape)\n\nplt.contourf(Z, extent=(mins[0], maxs[0], mins[1], maxs[1]),\n            cmap=\"copper\", alpha=0.6)\nplt.contour(Z, extent=(mins[0], maxs[0], mins[1], maxs[1]),\n            linewidths=1, colors='k')\n\n# plot data\nsns.scatterplot(data=coffee_imp, x='Aroma', y='Altitude Avg', s=20, hue='Variety', palette=sns.color_palette(\"Set2\", as_cmap=True))\n\n# plot centroids\ncentroids = kmeans.cluster_centers_\nplt.scatter(centroids[:, 0], centroids[:, 1],\n                marker='o', s=35, linewidths=8,\n                color='white', zorder=10, alpha=0.9)\nplt.scatter(centroids[:, 0], centroids[:, 1],\n                marker='x', s=2, linewidths=12,\n                color='black', zorder=11, alpha=1)\n\n\nC:\\Users\\Erica\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n  super()._check_params_vs_input(X, default_n_init=10)\n\n\n&lt;matplotlib.collections.PathCollection at 0x1fb869405f0&gt;\n\n\n\n\n\nThat does not look much better. Looks like we will need a better dataset to be able to do this, or to tweak the encoder enough that the values are more impactful."
  },
  {
    "objectID": "posts/clustering/index.html#step-1-import-libraries-and-coffee-dataset",
    "href": "posts/clustering/index.html#step-1-import-libraries-and-coffee-dataset",
    "title": "Clustering",
    "section": "",
    "text": "Code\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.metrics import r2_score\nfrom sklearn.cluster import KMeans\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.ensemble import RandomForestClassifier\nimport re\n\ncoffee_csv = 'https://raw.githubusercontent.com/erica16/ml1_blogs/main/data/df_arabica_clean.csv'\n\ncoffee_df = pd.read_csv(coffee_csv)"
  },
  {
    "objectID": "posts/clustering/index.html#step-2-get-familiar-with-the-data",
    "href": "posts/clustering/index.html#step-2-get-familiar-with-the-data",
    "title": "Clustering",
    "section": "",
    "text": "Before I do anything I need to know what type of data I’m dealing with here.\n\n\nCode\ncoffee_df.info()\n\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 207 entries, 0 to 206\nData columns (total 41 columns):\n #   Column                 Non-Null Count  Dtype  \n---  ------                 --------------  -----  \n 0   Unnamed: 0             207 non-null    int64  \n 1   ID                     207 non-null    int64  \n 2   Country of Origin      207 non-null    object \n 3   Farm Name              205 non-null    object \n 4   Lot Number             206 non-null    object \n 5   Mill                   204 non-null    object \n 6   ICO Number             75 non-null     object \n 7   Company                207 non-null    object \n 8   Altitude               206 non-null    object \n 9   Region                 205 non-null    object \n 10  Producer               206 non-null    object \n 11  Number of Bags         207 non-null    int64  \n 12  Bag Weight             207 non-null    object \n 13  In-Country Partner     207 non-null    object \n 14  Harvest Year           207 non-null    object \n 15  Grading Date           207 non-null    object \n 16  Owner                  207 non-null    object \n 17  Variety                201 non-null    object \n 18  Status                 207 non-null    object \n 19  Processing Method      202 non-null    object \n 20  Aroma                  207 non-null    float64\n 21  Flavor                 207 non-null    float64\n 22  Aftertaste             207 non-null    float64\n 23  Acidity                207 non-null    float64\n 24  Body                   207 non-null    float64\n 25  Balance                207 non-null    float64\n 26  Uniformity             207 non-null    float64\n 27  Clean Cup              207 non-null    float64\n 28  Sweetness              207 non-null    float64\n 29  Overall                207 non-null    float64\n 30  Defects                207 non-null    float64\n 31  Total Cup Points       207 non-null    float64\n 32  Moisture Percentage    207 non-null    float64\n 33  Category One Defects   207 non-null    int64  \n 34  Quakers                207 non-null    int64  \n 35  Color                  207 non-null    object \n 36  Category Two Defects   207 non-null    int64  \n 37  Expiration             207 non-null    object \n 38  Certification Body     207 non-null    object \n 39  Certification Address  207 non-null    object \n 40  Certification Contact  207 non-null    object \ndtypes: float64(13), int64(6), object(22)\nmemory usage: 66.4+ KB"
  },
  {
    "objectID": "posts/clustering/index.html#step-3-clean-data",
    "href": "posts/clustering/index.html#step-3-clean-data",
    "title": "Clustering",
    "section": "",
    "text": "A lot of these columns are not related to the coffee bean itself, so I’ll want to remove those. It also appears that I will need to handle null values, even after I take out the ICO number. I’ll go ahead and pare the dataset down to just the columns that are related to the coffee beans, but before I can remove null values I need to get the Altitude values from a string type to an int type so that I can use them more accurately. I wrote a function that uses regex to pull out the int values from the Altitude string into an array of stringified ints, which I turn into ints and then average so I only have one value for each Altitude. After that, I need the categorical columns to be useable by the machine learning algorithms, so I use a label encoder to turn the categorical data into numerical data. This dataset also isn’t very big, so I use an imputer to change null values to plausible values instead of dropping rows with null values.\n\n\nCode\n# clean data\ncoffee_df_small = coffee_df[['Country of Origin', 'Altitude', 'Region', 'Variety', 'Processing Method', 'Aroma', 'Flavor']].copy()\n\n# altitude has some values that are ranges, which is not ideal because it makes it a string not an int, so we'll take the average and replace the value with that\ndef get_altitude_int(alt_in):\n  ints = re.findall(r'\\b\\d+\\b', alt_in)\n  int_values = [int(num) for num in ints]\n  return int(np.mean(int_values))\n\naltitude_no_na = coffee_df_small['Altitude'].notnull()\ncoffee_df_small.loc[altitude_no_na, 'Altitude Avg'] = coffee_df_small.loc[altitude_no_na, 'Altitude'].apply(get_altitude_int)\n\n# turn categorical columns into numerical\ncoffee_df_nums = coffee_df_small.select_dtypes(include=['number'])\nlabel_encoder = LabelEncoder()\n\nfor col in coffee_df_small.columns:\n    if coffee_df_small[col].dtype == 'object':  # Check if column has categorical data\n        coffee_df_nums[col] = label_encoder.fit_transform(coffee_df_small[col])\n\nimputer = SimpleImputer(strategy=\"median\")\nimputer.fit(coffee_df_nums)\ncoffee_imp = pd.DataFrame(imputer.transform(coffee_df_nums), columns=coffee_df_nums.columns)\nprint(coffee_imp.info())\n\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 207 entries, 0 to 206\nData columns (total 8 columns):\n #   Column             Non-Null Count  Dtype  \n---  ------             --------------  -----  \n 0   Aroma              207 non-null    float64\n 1   Flavor             207 non-null    float64\n 2   Altitude Avg       207 non-null    float64\n 3   Country of Origin  207 non-null    float64\n 4   Altitude           207 non-null    float64\n 5   Region             207 non-null    float64\n 6   Variety            207 non-null    float64\n 7   Processing Method  207 non-null    float64\ndtypes: float64(8)\nmemory usage: 13.1 KB\nNone"
  },
  {
    "objectID": "posts/clustering/index.html#step-4-cleaning-is-done-time-to-investigate-feature-importance",
    "href": "posts/clustering/index.html#step-4-cleaning-is-done-time-to-investigate-feature-importance",
    "title": "Clustering",
    "section": "",
    "text": "I am going to use a random forest classifier to determine what features are most relevant to predicting coffee variety.\n\n\nCode\n# do random forest to find out what the most important features are\ncoffee_enc_copy = coffee_imp.copy()\ntrain_set_X, test_set_X, train_set_y, test_set_y = train_test_split(coffee_enc_copy.drop('Variety', axis='columns'), coffee_enc_copy['Variety'], test_size=0.2, random_state=42)\n\nrnd_clf_coffee = RandomForestClassifier(n_estimators=500, max_leaf_nodes=16,\n                                 n_jobs=-1, random_state=42)\nrnd_clf_coffee.fit(train_set_X, train_set_y)\n\n# look at the relationship\nfeature_importances = pd.Series(rnd_clf_coffee.feature_importances_, train_set_X.columns).sort_values()\n\nplt.barh(feature_importances.index, feature_importances.values * 100, color='brown')\nplt.ylabel('Feature')\nplt.xlabel('Percentage Feature Importance')\nplt.title('Feature Importance of Coffee Features to Coffee Variety')\nplt.show()\n\n\n\n\n\nThere are a couple interesting things to note in this bar chart. The first is the order of the important features: altitude average is most important at a little over 20%, then Country of Origin at around 20%. The second is that altitude average is more important than altitude, but I created altitude average from altitude, so shouldn’t they be the same? The reason they are not is that the label encoder assumes there is no relation between the unique values in the column, so a super high altitude could be labeled 0 and a super low altitude could be labeled 207 (since that is the number of items in each column). Clearly, being able to understand that altitude is a numerical value is important to this model. With that in mind, that also means that country of origin might be less useable after being run through a label encoder than if I had found a way of imparting relative geographic location into the numerical representation of country of origin."
  },
  {
    "objectID": "posts/clustering/index.html#step-5-run-a-k-means-on-the-top-2-most-important-coffee-features-to-variety",
    "href": "posts/clustering/index.html#step-5-run-a-k-means-on-the-top-2-most-important-coffee-features-to-variety",
    "title": "Clustering",
    "section": "",
    "text": "The top 2 features are altitude average and country of origin, so naturally we will do our K-Means clustering algorithm on those variables. I want altitude on the y axis because it is the vertical column and is a good representation of actual altitude. I will also use 7 clusters because that is how many varieties of coffee there are in this dataset.\n\n\nCode\n# aroma_alt_np = np.array(coffee_imp[['Aroma', 'Altitude Avg']])\naltitude_country_np = np.array(coffee_imp[['Country of Origin', 'Altitude Avg']])\n# k-means clusterer\nk = 7\nkmeans = KMeans(n_clusters=k, random_state=42)\ny_pred = kmeans.fit_predict(altitude_country_np)\n\n# plot decision boundaries\nmins = altitude_country_np.min(axis=0) - 0.1\nmaxs = altitude_country_np.max(axis=0) + 0.1\nxx, yy = np.meshgrid(np.linspace(mins[0], maxs[0], 1000),\n                      np.linspace(mins[1], maxs[1], 1000))\nZ = kmeans.predict(np.c_[xx.ravel(), yy.ravel()])\nZ = Z.reshape(xx.shape)\n\nplt.contourf(Z, extent=(mins[0], maxs[0], mins[1], maxs[1]),\n            cmap=\"copper\", alpha=0.6)\nplt.contour(Z, extent=(mins[0], maxs[0], mins[1], maxs[1]),\n            linewidths=1, colors='k')\n\n# plot data\nsns.scatterplot(data=coffee_imp, x='Country of Origin', y='Altitude Avg', s=20, hue='Variety', palette=sns.color_palette(\"Set2\", as_cmap=True))\n\n# plot centroids\ncentroids = kmeans.cluster_centers_\nplt.scatter(centroids[:, 0], centroids[:, 1],\n                marker='o', s=35, linewidths=8,\n                color='white', zorder=10, alpha=0.9)\nplt.scatter(centroids[:, 0], centroids[:, 1],\n                marker='x', s=2, linewidths=12,\n                color='black', zorder=11, alpha=1)\n\n\nC:\\Users\\Erica\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n  super()._check_params_vs_input(X, default_n_init=10)\n\n\n&lt;matplotlib.collections.PathCollection at 0x1fb83bf7740&gt;"
  },
  {
    "objectID": "posts/clustering/index.html#step-6-reevaluate",
    "href": "posts/clustering/index.html#step-6-reevaluate",
    "title": "Clustering",
    "section": "",
    "text": "That chart looks… not great. One reason could be because Country of Origin was originally categorical, and the label encoder may have not done a great job. Just for fun, we’ll take the feature with the highest feature importance to variety that started out as numerical - aroma. That is how people can tell between coffee beans, lets hope the same goes for a K-Means clustering algorithm!\n\n\nCode\naroma_alt_np = np.array(coffee_imp[['Aroma', 'Altitude Avg']])\n# k-means clusterer\nk = 7\nkmeans = KMeans(n_clusters=k, random_state=42)\ny_pred = kmeans.fit_predict(aroma_alt_np)\n\n# plot decision boundaries\nmins = aroma_alt_np.min(axis=0) - 0.1\nmaxs = aroma_alt_np.max(axis=0) + 0.1\nxx, yy = np.meshgrid(np.linspace(mins[0], maxs[0], 1000),\n                      np.linspace(mins[1], maxs[1], 1000))\nZ = kmeans.predict(np.c_[xx.ravel(), yy.ravel()])\nZ = Z.reshape(xx.shape)\n\nplt.contourf(Z, extent=(mins[0], maxs[0], mins[1], maxs[1]),\n            cmap=\"copper\", alpha=0.6)\nplt.contour(Z, extent=(mins[0], maxs[0], mins[1], maxs[1]),\n            linewidths=1, colors='k')\n\n# plot data\nsns.scatterplot(data=coffee_imp, x='Aroma', y='Altitude Avg', s=20, hue='Variety', palette=sns.color_palette(\"Set2\", as_cmap=True))\n\n# plot centroids\ncentroids = kmeans.cluster_centers_\nplt.scatter(centroids[:, 0], centroids[:, 1],\n                marker='o', s=35, linewidths=8,\n                color='white', zorder=10, alpha=0.9)\nplt.scatter(centroids[:, 0], centroids[:, 1],\n                marker='x', s=2, linewidths=12,\n                color='black', zorder=11, alpha=1)\n\n\nC:\\Users\\Erica\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n  super()._check_params_vs_input(X, default_n_init=10)\n\n\n&lt;matplotlib.collections.PathCollection at 0x1fb869405f0&gt;\n\n\n\n\n\nThat does not look much better. Looks like we will need a better dataset to be able to do this, or to tweak the encoder enough that the values are more impactful."
  },
  {
    "objectID": "posts/outlier-detection/index.html",
    "href": "posts/outlier-detection/index.html",
    "title": "Quarto Basics",
    "section": "",
    "text": "As someone who majored in Environmental Informatics for my undergrad at Virginia Tech, exploring climate data and its relationship to non-climate datasets is something that I am familiar with and very passionate about. I wanted to make sure I had a set of data to use for a study on anomaly and outlier detection that would show a strong correlation between the two variables, but maybe not one that would come straight to mind for the average person. I had a hunch that the Inequality index for a country and the CO2 emissions for a country might be correlated, so I found a C02 emissions dataset from https://www.kaggle.com/datasets/ankanhore545/carbon-dioxide-emissions-of-the-world and an Economic Inequality dataset from https://ourworldindata.org/economic-inequality.\n\n\n\nC02, or carbon dioxide, is one of the primary greenhouse gases responsible for driving climate change. As a greenhouse gas, its presence traps infrared radiation (heat) in the atmosphere which keeps earth comfortable in natural amounts but is detrimental when the amounts are much higher than they would normally be due to human-caused spikes in CO2 emissions. Massive increases in carbon dioxide emissions are primarily due to burning fossil fuels and other biomass-based fuels for energy used in industrial, housing, and transportation settings, and large changes in land use patterns. Income Inequality per country is measured with the Gini Coefficient, which is a measure of the income inequality from 0 to 1 where 0 is perfect income equality and 1 is the maximal inequality. It helps assess how evenly or unevenly the wealth in a population is distributed. It is calculated by measuring a value like income on a frequency distribution, where a 45 degree angle would indicate perfect wealth distribution.\n\n\n\nThe first thing to do after downloading the datasets and hosting them on my git repo was to explore the data, and the best way to do that is to get it onto a visualization. Before being able to visualize it though, the data needs to be cleaned - in this case, that means removing rows without useable data and making the dataframes easier to use by filtering out unnecessary columns. After the data is presentable in two separate dataframes, it needs to be merged into the same dataframe so that it can be graphed. I merged the CO2 Emissions dataframe and the Income Inequality index dataframe on Country after filtering for just rows in the Inequality dataframe where the year was 2018. I chose 2018 because it was the most recent year with data in both datasets.\n\n\nCode\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.cluster import KMeans\n\nemissions_csv = 'https://raw.githubusercontent.com/erica16/ml1_blogs/main/data/historical_emissions.csv'\ninequality_csv ='https://raw.githubusercontent.com/erica16/ml1_blogs/main/data/inequality.csv'\n\n#read in datasets\nemissions_df = pd.read_csv(emissions_csv)\ninequality_df = pd.read_csv(inequality_csv)\n\n#clean data\ninequality_filtered = inequality_df.filter(['Country', 'Year', 'Gini coefficient (before tax) (World Inequality Database)'])\n\nprint(emissions_df.info())\n\n#filter to 2018\ninequality_2018 = inequality_filtered.loc[inequality_filtered['Year'] == 2018]\nprint(inequality_2018.info())\n\n#merge dfs on country\ninequality_emissions_df = inequality_2018.merge(emissions_df, on='Country')\nin_em_nums = inequality_emissions_df.select_dtypes(include=[np.number])\nimputer = SimpleImputer(strategy=\"median\")\nimputer.fit(in_em_nums)\nin_em_imputed = imputer.transform(in_em_nums)\ninequality_emissions_df[in_em_nums.columns] = in_em_imputed\n\n\n# Visualize the data\nsns.scatterplot(data=inequality_emissions_df, x='Gini coefficient (before tax) (World Inequality Database)' ,y='2018')\nplt.xlabel('Inequality index (Gini Coefficient, before tax)')\nplt.ylabel('CO2 Emissions in MtCO₂e')\nplt.title('Correlation between CO2 Emissions and Inequality metric by Country in 2018')\nplt.show()\n\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 195 entries, 0 to 194\nData columns (total 34 columns):\n #   Column       Non-Null Count  Dtype  \n---  ------       --------------  -----  \n 0   Country      195 non-null    object \n 1   Data source  195 non-null    object \n 2   Sector       195 non-null    object \n 3   Gas          195 non-null    object \n 4   Unit         195 non-null    object \n 5   2018         195 non-null    float64\n 6   2017         195 non-null    float64\n 7   2016         195 non-null    float64\n 8   2015         195 non-null    float64\n 9   2014         195 non-null    float64\n 10  2013         195 non-null    float64\n 11  2012         195 non-null    float64\n 12  2011         195 non-null    float64\n 13  2010         195 non-null    float64\n 14  2009         195 non-null    float64\n 15  2008         195 non-null    float64\n 16  2007         195 non-null    float64\n 17  2006         195 non-null    float64\n 18  2005         195 non-null    float64\n 19  2004         195 non-null    float64\n 20  2003         195 non-null    float64\n 21  2002         195 non-null    float64\n 22  2001         195 non-null    float64\n 23  2000         195 non-null    float64\n 24  1999         195 non-null    float64\n 25  1998         195 non-null    float64\n 26  1997         195 non-null    float64\n 27  1996         195 non-null    float64\n 28  1995         195 non-null    float64\n 29  1994         195 non-null    float64\n 30  1993         195 non-null    float64\n 31  1992         195 non-null    float64\n 32  1991         195 non-null    float64\n 33  1990         194 non-null    float64\ndtypes: float64(29), object(5)\nmemory usage: 51.9+ KB\nNone\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nIndex: 210 entries, 38 to 9923\nData columns (total 3 columns):\n #   Column                                                     Non-Null Count  Dtype  \n---  ------                                                     --------------  -----  \n 0   Country                                                    210 non-null    object \n 1   Year                                                       210 non-null    int64  \n 2   Gini coefficient (before tax) (World Inequality Database)  80 non-null     float64\ndtypes: float64(1), int64(1), object(1)\nmemory usage: 6.6+ KB\nNone\n\n\n\n\n\n\n\n\nLooking at this scatterplot, 2 things are immediately obvious: the first is that almost all the points are clustered towards the bottom of the Y axis, but are clustered nonetheless, and the second is that there is one visually obvious outlier and one point that could be an outlier but requires further investigation.\n\n\n\nIn order to decide which of these points are truly outliers, I will use a K-means cluster with only 2 clusters, and the outcome will show the majority of the dataset in one cluster and the outliers in the other cluster.\n\n\nCode\n# k-means clusterer\nineq_emiss_np = np.array(inequality_emissions_df[['Gini coefficient (before tax) (World Inequality Database)', '2018']])\n\nk = 2\nkmeans = KMeans(n_clusters=k, random_state=42)\ny_pred = kmeans.fit_predict(ineq_emiss_np)\n\n# plot decision boundaries\nmins = ineq_emiss_np.min(axis=0) - 0.1\nmaxs = ineq_emiss_np.max(axis=0) + 0.1\nxx, yy = np.meshgrid(np.linspace(mins[0], maxs[0], 1000),\n                      np.linspace(mins[1], maxs[1], 1000))\nZ = kmeans.predict(np.c_[xx.ravel(), yy.ravel()])\nZ = Z.reshape(xx.shape)\n\nplt.contourf(Z, extent=(mins[0], maxs[0], mins[1], maxs[1]),\n            cmap='Set3')\nplt.contour(Z, extent=(mins[0], maxs[0], mins[1], maxs[1]),\n            linewidths=1, colors='k')\n\n# plot data\nsns.scatterplot(data=inequality_emissions_df, x='Gini coefficient (before tax) (World Inequality Database)', y='2018', s = 100)\n\nplt.xlabel('Inequality index (Gini Coefficient, before tax)')\nplt.ylabel('CO2 Emissions in MtCO₂e')\nplt.title('Correlation between CO2 Emissions and Inequality metric by Country in 2018')\nplt.show()\n\n\nC:\\Users\\Erica\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n  super()._check_params_vs_input(X, default_n_init=10)\n\n\n\n\n\nBased on this graph, the yellow cluster at the top holds the single outlier for the dataset, while the bottom turquoise cluster holds the valid data for the dataset, including the two points we weren’t totally sure were going to be part of the valid dataset. This goes to show that a machine learning algorithm is a great way to determine what points are outliers, especially when it is dubious upon visual inspection. If we want to be a little bit more discerning with the outlier detection, we could try for 3 clusters.\n\n\nCode\n# k-means clusterer\nineq_emiss_np = np.array(inequality_emissions_df[['Gini coefficient (before tax) (World Inequality Database)', '2018']])\n\nk = 3\nkmeans = KMeans(n_clusters=k, random_state=42)\ny_pred = kmeans.fit_predict(ineq_emiss_np)\n\n# plot decision boundaries\nmins = ineq_emiss_np.min(axis=0) - 0.1\nmaxs = ineq_emiss_np.max(axis=0) + 0.1\nxx, yy = np.meshgrid(np.linspace(mins[0], maxs[0], 1000),\n                      np.linspace(mins[1], maxs[1], 1000))\nZ = kmeans.predict(np.c_[xx.ravel(), yy.ravel()])\nZ = Z.reshape(xx.shape)\n\nplt.contourf(Z, extent=(mins[0], maxs[0], mins[1], maxs[1]),\n            cmap='Set3')\nplt.contour(Z, extent=(mins[0], maxs[0], mins[1], maxs[1]),\n            linewidths=1, colors='k')\n\n# plot data\nsns.scatterplot(data=inequality_emissions_df, x='Gini coefficient (before tax) (World Inequality Database)', y='2018', s = 100)\n\nplt.xlabel('Inequality index (Gini Coefficient, before tax)')\nplt.ylabel('CO2 Emissions in MtCO₂e')\nplt.title('Correlation between CO2 Emissions and Inequality metric by Country in 2018')\nplt.show()\n\n\nC:\\Users\\Erica\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n  super()._check_params_vs_input(X, default_n_init=10)\n\n\n\n\n\nHere we can see that the outlier from the two cluster graph is still in its own cluster, but the point second-furthest away from the majority of the group is now in its own cluster, and the point third-furthest away seems to be straddling the line between possible outlier and valid data point. The cluster containing the majority of the points is the one containing the valid data."
  },
  {
    "objectID": "posts/outlier-detection/index.html#on-the-correlation-between-co2-emissions-and-economic-inequality-metric-by-country",
    "href": "posts/outlier-detection/index.html#on-the-correlation-between-co2-emissions-and-economic-inequality-metric-by-country",
    "title": "Quarto Basics",
    "section": "",
    "text": "As someone who majored in Environmental Informatics for my undergrad at Virginia Tech, exploring climate data and its relationship to non-climate datasets is something that I am familiar with and very passionate about. I wanted to make sure I had a set of data to use for a study on anomaly and outlier detection that would show a strong correlation between the two variables, but maybe not one that would come straight to mind for the average person. I had a hunch that the Inequality index for a country and the CO2 emissions for a country might be correlated, so I found a C02 emissions dataset from https://www.kaggle.com/datasets/ankanhore545/carbon-dioxide-emissions-of-the-world and an Economic Inequality dataset from https://ourworldindata.org/economic-inequality."
  },
  {
    "objectID": "posts/outlier-detection/index.html#what-are-these-variables",
    "href": "posts/outlier-detection/index.html#what-are-these-variables",
    "title": "Quarto Basics",
    "section": "",
    "text": "C02, or carbon dioxide, is one of the primary greenhouse gases responsible for driving climate change. As a greenhouse gas, its presence traps infrared radiation (heat) in the atmosphere which keeps earth comfortable in natural amounts but is detrimental when the amounts are much higher than they would normally be due to human-caused spikes in CO2 emissions. Massive increases in carbon dioxide emissions are primarily due to burning fossil fuels and other biomass-based fuels for energy used in industrial, housing, and transportation settings, and large changes in land use patterns. Income Inequality per country is measured with the Gini Coefficient, which is a measure of the income inequality from 0 to 1 where 0 is perfect income equality and 1 is the maximal inequality. It helps assess how evenly or unevenly the wealth in a population is distributed. It is calculated by measuring a value like income on a frequency distribution, where a 45 degree angle would indicate perfect wealth distribution."
  },
  {
    "objectID": "posts/outlier-detection/index.html#step-1-clean-data-and-view-the-basic-scatterplot",
    "href": "posts/outlier-detection/index.html#step-1-clean-data-and-view-the-basic-scatterplot",
    "title": "Quarto Basics",
    "section": "",
    "text": "The first thing to do after downloading the datasets and hosting them on my git repo was to explore the data, and the best way to do that is to get it onto a visualization. Before being able to visualize it though, the data needs to be cleaned - in this case, that means removing rows without useable data and making the dataframes easier to use by filtering out unnecessary columns. After the data is presentable in two separate dataframes, it needs to be merged into the same dataframe so that it can be graphed. I merged the CO2 Emissions dataframe and the Income Inequality index dataframe on Country after filtering for just rows in the Inequality dataframe where the year was 2018. I chose 2018 because it was the most recent year with data in both datasets.\n\n\nCode\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.cluster import KMeans\n\nemissions_csv = 'https://raw.githubusercontent.com/erica16/ml1_blogs/main/data/historical_emissions.csv'\ninequality_csv ='https://raw.githubusercontent.com/erica16/ml1_blogs/main/data/inequality.csv'\n\n#read in datasets\nemissions_df = pd.read_csv(emissions_csv)\ninequality_df = pd.read_csv(inequality_csv)\n\n#clean data\ninequality_filtered = inequality_df.filter(['Country', 'Year', 'Gini coefficient (before tax) (World Inequality Database)'])\n\nprint(emissions_df.info())\n\n#filter to 2018\ninequality_2018 = inequality_filtered.loc[inequality_filtered['Year'] == 2018]\nprint(inequality_2018.info())\n\n#merge dfs on country\ninequality_emissions_df = inequality_2018.merge(emissions_df, on='Country')\nin_em_nums = inequality_emissions_df.select_dtypes(include=[np.number])\nimputer = SimpleImputer(strategy=\"median\")\nimputer.fit(in_em_nums)\nin_em_imputed = imputer.transform(in_em_nums)\ninequality_emissions_df[in_em_nums.columns] = in_em_imputed\n\n\n# Visualize the data\nsns.scatterplot(data=inequality_emissions_df, x='Gini coefficient (before tax) (World Inequality Database)' ,y='2018')\nplt.xlabel('Inequality index (Gini Coefficient, before tax)')\nplt.ylabel('CO2 Emissions in MtCO₂e')\nplt.title('Correlation between CO2 Emissions and Inequality metric by Country in 2018')\nplt.show()\n\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 195 entries, 0 to 194\nData columns (total 34 columns):\n #   Column       Non-Null Count  Dtype  \n---  ------       --------------  -----  \n 0   Country      195 non-null    object \n 1   Data source  195 non-null    object \n 2   Sector       195 non-null    object \n 3   Gas          195 non-null    object \n 4   Unit         195 non-null    object \n 5   2018         195 non-null    float64\n 6   2017         195 non-null    float64\n 7   2016         195 non-null    float64\n 8   2015         195 non-null    float64\n 9   2014         195 non-null    float64\n 10  2013         195 non-null    float64\n 11  2012         195 non-null    float64\n 12  2011         195 non-null    float64\n 13  2010         195 non-null    float64\n 14  2009         195 non-null    float64\n 15  2008         195 non-null    float64\n 16  2007         195 non-null    float64\n 17  2006         195 non-null    float64\n 18  2005         195 non-null    float64\n 19  2004         195 non-null    float64\n 20  2003         195 non-null    float64\n 21  2002         195 non-null    float64\n 22  2001         195 non-null    float64\n 23  2000         195 non-null    float64\n 24  1999         195 non-null    float64\n 25  1998         195 non-null    float64\n 26  1997         195 non-null    float64\n 27  1996         195 non-null    float64\n 28  1995         195 non-null    float64\n 29  1994         195 non-null    float64\n 30  1993         195 non-null    float64\n 31  1992         195 non-null    float64\n 32  1991         195 non-null    float64\n 33  1990         194 non-null    float64\ndtypes: float64(29), object(5)\nmemory usage: 51.9+ KB\nNone\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nIndex: 210 entries, 38 to 9923\nData columns (total 3 columns):\n #   Column                                                     Non-Null Count  Dtype  \n---  ------                                                     --------------  -----  \n 0   Country                                                    210 non-null    object \n 1   Year                                                       210 non-null    int64  \n 2   Gini coefficient (before tax) (World Inequality Database)  80 non-null     float64\ndtypes: float64(1), int64(1), object(1)\nmemory usage: 6.6+ KB\nNone"
  },
  {
    "objectID": "posts/outlier-detection/index.html#step-2-evaluate-scatterplot-findings",
    "href": "posts/outlier-detection/index.html#step-2-evaluate-scatterplot-findings",
    "title": "Quarto Basics",
    "section": "",
    "text": "Looking at this scatterplot, 2 things are immediately obvious: the first is that almost all the points are clustered towards the bottom of the Y axis, but are clustered nonetheless, and the second is that there is one visually obvious outlier and one point that could be an outlier but requires further investigation."
  },
  {
    "objectID": "posts/outlier-detection/index.html#step-3-k-means-clustering-to-determine-outliers",
    "href": "posts/outlier-detection/index.html#step-3-k-means-clustering-to-determine-outliers",
    "title": "Quarto Basics",
    "section": "",
    "text": "In order to decide which of these points are truly outliers, I will use a K-means cluster with only 2 clusters, and the outcome will show the majority of the dataset in one cluster and the outliers in the other cluster.\n\n\nCode\n# k-means clusterer\nineq_emiss_np = np.array(inequality_emissions_df[['Gini coefficient (before tax) (World Inequality Database)', '2018']])\n\nk = 2\nkmeans = KMeans(n_clusters=k, random_state=42)\ny_pred = kmeans.fit_predict(ineq_emiss_np)\n\n# plot decision boundaries\nmins = ineq_emiss_np.min(axis=0) - 0.1\nmaxs = ineq_emiss_np.max(axis=0) + 0.1\nxx, yy = np.meshgrid(np.linspace(mins[0], maxs[0], 1000),\n                      np.linspace(mins[1], maxs[1], 1000))\nZ = kmeans.predict(np.c_[xx.ravel(), yy.ravel()])\nZ = Z.reshape(xx.shape)\n\nplt.contourf(Z, extent=(mins[0], maxs[0], mins[1], maxs[1]),\n            cmap='Set3')\nplt.contour(Z, extent=(mins[0], maxs[0], mins[1], maxs[1]),\n            linewidths=1, colors='k')\n\n# plot data\nsns.scatterplot(data=inequality_emissions_df, x='Gini coefficient (before tax) (World Inequality Database)', y='2018', s = 100)\n\nplt.xlabel('Inequality index (Gini Coefficient, before tax)')\nplt.ylabel('CO2 Emissions in MtCO₂e')\nplt.title('Correlation between CO2 Emissions and Inequality metric by Country in 2018')\nplt.show()\n\n\nC:\\Users\\Erica\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n  super()._check_params_vs_input(X, default_n_init=10)\n\n\n\n\n\nBased on this graph, the yellow cluster at the top holds the single outlier for the dataset, while the bottom turquoise cluster holds the valid data for the dataset, including the two points we weren’t totally sure were going to be part of the valid dataset. This goes to show that a machine learning algorithm is a great way to determine what points are outliers, especially when it is dubious upon visual inspection. If we want to be a little bit more discerning with the outlier detection, we could try for 3 clusters.\n\n\nCode\n# k-means clusterer\nineq_emiss_np = np.array(inequality_emissions_df[['Gini coefficient (before tax) (World Inequality Database)', '2018']])\n\nk = 3\nkmeans = KMeans(n_clusters=k, random_state=42)\ny_pred = kmeans.fit_predict(ineq_emiss_np)\n\n# plot decision boundaries\nmins = ineq_emiss_np.min(axis=0) - 0.1\nmaxs = ineq_emiss_np.max(axis=0) + 0.1\nxx, yy = np.meshgrid(np.linspace(mins[0], maxs[0], 1000),\n                      np.linspace(mins[1], maxs[1], 1000))\nZ = kmeans.predict(np.c_[xx.ravel(), yy.ravel()])\nZ = Z.reshape(xx.shape)\n\nplt.contourf(Z, extent=(mins[0], maxs[0], mins[1], maxs[1]),\n            cmap='Set3')\nplt.contour(Z, extent=(mins[0], maxs[0], mins[1], maxs[1]),\n            linewidths=1, colors='k')\n\n# plot data\nsns.scatterplot(data=inequality_emissions_df, x='Gini coefficient (before tax) (World Inequality Database)', y='2018', s = 100)\n\nplt.xlabel('Inequality index (Gini Coefficient, before tax)')\nplt.ylabel('CO2 Emissions in MtCO₂e')\nplt.title('Correlation between CO2 Emissions and Inequality metric by Country in 2018')\nplt.show()\n\n\nC:\\Users\\Erica\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n  super()._check_params_vs_input(X, default_n_init=10)\n\n\n\n\n\nHere we can see that the outlier from the two cluster graph is still in its own cluster, but the point second-furthest away from the majority of the group is now in its own cluster, and the point third-furthest away seems to be straddling the line between possible outlier and valid data point. The cluster containing the majority of the points is the one containing the valid data."
  }
]
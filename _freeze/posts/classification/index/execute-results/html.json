{
  "hash": "45e6702961776831272075263cd0afab",
  "result": {
    "markdown": "---\ntitle: Random Forest Classifier\nformat:\n  html:\n    code-fold: true\neditor:\n  render-on-save: true\n---\n\n# Random Forest Classifier\nIn college we are told that if we spend a lot of time studying, make sure to listen well in class, make sure to GO to class, and take good notes we will succeed. But does this advice hold up to scrutiny? We are about to find out. Using a dataset that has a wide range of metrics on undergraduate college students, I will use a random forest classifier to determine what features are most important to a student's grade.\n\ndatset: https://www.kaggle.com/datasets/jacksondivakarr/student-classification-dataset/data\n\n## Step 1: Import libraries and dataset\n\n::: {.cell execution_count=1}\n``` {.python .cell-code}\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.compose import make_column_selector\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.model_selection import train_test_split\n\n# get data\nstudent_df = pd.read_csv('https://raw.githubusercontent.com/erica16/ml1_blogs/main/data/student.csv')\n\nprint(student_df.head())\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n   Unnamed: 0    Id  Student_Age     Sex High_School_Type Scholarship  \\\n0           0  5001           21    Male            Other         50%   \n1           1  5002           20    Male            Other         50%   \n2           2  5003           21    Male            State         50%   \n3           3  5004           18  Female          Private         50%   \n4           4  5005           22    Male          Private         50%   \n\n  Additional_Work Sports_activity Transportation  Weekly_Study_Hours  \\\n0             Yes              No        Private                   0   \n1             Yes              No        Private                   0   \n2              No              No        Private                   2   \n3             Yes              No            Bus                   2   \n4              No              No            Bus                  12   \n\n  Attendance Reading Notes Listening_in_Class Project_work Grade  \n0     Always     Yes   Yes                 No           No    AA  \n1     Always     Yes    No                Yes          Yes    AA  \n2      Never      No    No                 No          Yes    AA  \n3     Always      No   Yes                 No           No    AA  \n4     Always     Yes    No                Yes          Yes    AA  \n```\n:::\n:::\n\n\n## Step 2: Clean data\nSince a lot of this data is categorical, we will need to use an encoder to make it numerical so the random forest classifier can handle it. For this task I am using a label encoder because the majority of the categorical data is not ordered and has a small set of repeated values.\n\n::: {.cell execution_count=2}\n``` {.python .cell-code}\n#dropping ID because it will not be used to train or be predicted\nstudent_df_clean = student_df.drop(student_df.columns[:2], axis='columns')\ncat_selector = make_column_selector(dtype_include=object)\ncat_columns = cat_selector(student_df_clean)\n\n# use encoder for categorical data\nstudent_df_enc = student_df_clean.select_dtypes(include=['number'])\nlabel_encoder = LabelEncoder()\n\n# Iterate through columns and encode categorical columns\nfor col in student_df_clean.columns:\n    if student_df_clean[col].dtype == 'object':  # Check if column has categorical data\n        student_df_enc[col] = label_encoder.fit_transform(student_df_clean[col])\n\nprint(student_df_enc.head())\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n   Student_Age  Weekly_Study_Hours  Sex  High_School_Type  Scholarship  \\\n0           21                   0    1                 0            2   \n1           20                   0    1                 0            2   \n2           21                   2    1                 2            2   \n3           18                   2    0                 1            2   \n4           22                  12    1                 1            2   \n\n   Additional_Work  Sports_activity  Transportation  Attendance  Reading  \\\n0                1                0               1           1        1   \n1                1                0               1           1        1   \n2                0                0               1           2        0   \n3                1                0               0           1        0   \n4                0                0               0           1        1   \n\n   Notes  Listening_in_Class  Project_work  Grade  \n0      2                   1             0      0  \n1      1                   2             1      0  \n2      1                   1             1      0  \n3      2                   1             0      0  \n4      1                   2             1      0  \n```\n:::\n:::\n\n\n## Step 3: Train and run random forest classifier\nNow that the data is clean we can split it up into training and testing data using the scikit learn train_test_split function. Since we want to know what factors impact grade, the independent variables (X) will be all the factors besides grade, and the dependent variable (y) will be grade.\n\n::: {.cell execution_count=3}\n``` {.python .cell-code}\ntrain_set_X, test_set_X, train_set_y, test_set_y = train_test_split(student_df_enc.drop('Grade', axis='columns'), student_df_enc['Grade'], test_size=0.2, random_state=42)\n\nrnd_clf = RandomForestClassifier(n_estimators=500, max_leaf_nodes=16,\n                                 n_jobs=-1, random_state=42)\nrnd_clf.fit(train_set_X, train_set_y)\n\ny_pred_rf = rnd_clf.predict(test_set_X)\n\n# take a look at feature importances to see what to use as the independent value\n\nfeature_importances = pd.Series(rnd_clf.feature_importances_, train_set_X.columns).sort_values()\n\nplt.barh(feature_importances.index, feature_importances.values * 100)\nplt.xlabel('Percent Impact on Grade')\nplt.ylabel('Student Factor')\nplt.title('Impact on Student Activities on Grade')\nplt.show()\n\nprint(f\"Accuracy score: {accuracy_score(test_set_y, y_pred_rf) * 100}%\")\n```\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-4-output-1.png){width=706 height=449}\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\nAccuracy score: 20.689655172413794%\n```\n:::\n:::\n\n\n## Investigate results\nUpon viewing the top factors that impact a student's grade, I am compelled to remind the reader that correlation does not mean causation. Surely there are other forces at play that make a student's age the number one most impactful factor on their grade, and whether they do or don't have a scholarship also probably isn't the cause of good grades. It is also important to keep an eye on the scale of this chart - the highest factor is only about 14% impactful, and the highest factor that a student can actually control is about 9% impactful. It doesn't look like any one thing will have a huge impact on grades, which makes sense. If a student starts going to class more, for example, but doesn't pay attention or take notes, that most likely won't cause a major impact in their grade (unless their grade is 100% attendance based, but that is also unlikely).\nThe other thing to point out is the accuracy score - this random forest classifier is only 20% accurate. That means there is a lot of room for improvement! Perhaps more data, or better quality data, or more tweaking the label encoder could increase this number.\n\n",
    "supporting": [
      "index_files"
    ],
    "filters": [],
    "includes": {}
  }
}
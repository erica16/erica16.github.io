{
  "hash": "676ab0e283f5e5b638c18f107f8dc501",
  "result": {
    "markdown": "---\ntitle: Clustering\nformat:\n  html:\n    code-fold: true\neditor:\n  render-on-save: true\n---\n\n# Clustering\nAs an avid coffee drinker, I have always been impressed by people who can give a cup of black coffee a good sniff and tell you exactly where it was grown or the bean variety. I want to know if a machine learning algorithm could \"smell out\" coffee bean variety based on several factors. \nData set source: https://www.kaggle.com/datasets/fatihb/coffee-quality-data-cqi\n\n## Step 1: Import libraries and coffee dataset\n\n::: {.cell execution_count=1}\n``` {.python .cell-code}\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.metrics import r2_score\nfrom sklearn.cluster import KMeans\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.ensemble import RandomForestClassifier\nimport re\n\ncoffee_csv = 'https://raw.githubusercontent.com/erica16/ml1_blogs/main/data/df_arabica_clean.csv'\n\ncoffee_df = pd.read_csv(coffee_csv)\n```\n:::\n\n\n## Step 2: Get familiar with the data\nBefore I do anything I need to know what type of data I'm dealing with here.\n\n::: {.cell execution_count=2}\n``` {.python .cell-code}\ncoffee_df.info()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 207 entries, 0 to 206\nData columns (total 41 columns):\n #   Column                 Non-Null Count  Dtype  \n---  ------                 --------------  -----  \n 0   Unnamed: 0             207 non-null    int64  \n 1   ID                     207 non-null    int64  \n 2   Country of Origin      207 non-null    object \n 3   Farm Name              205 non-null    object \n 4   Lot Number             206 non-null    object \n 5   Mill                   204 non-null    object \n 6   ICO Number             75 non-null     object \n 7   Company                207 non-null    object \n 8   Altitude               206 non-null    object \n 9   Region                 205 non-null    object \n 10  Producer               206 non-null    object \n 11  Number of Bags         207 non-null    int64  \n 12  Bag Weight             207 non-null    object \n 13  In-Country Partner     207 non-null    object \n 14  Harvest Year           207 non-null    object \n 15  Grading Date           207 non-null    object \n 16  Owner                  207 non-null    object \n 17  Variety                201 non-null    object \n 18  Status                 207 non-null    object \n 19  Processing Method      202 non-null    object \n 20  Aroma                  207 non-null    float64\n 21  Flavor                 207 non-null    float64\n 22  Aftertaste             207 non-null    float64\n 23  Acidity                207 non-null    float64\n 24  Body                   207 non-null    float64\n 25  Balance                207 non-null    float64\n 26  Uniformity             207 non-null    float64\n 27  Clean Cup              207 non-null    float64\n 28  Sweetness              207 non-null    float64\n 29  Overall                207 non-null    float64\n 30  Defects                207 non-null    float64\n 31  Total Cup Points       207 non-null    float64\n 32  Moisture Percentage    207 non-null    float64\n 33  Category One Defects   207 non-null    int64  \n 34  Quakers                207 non-null    int64  \n 35  Color                  207 non-null    object \n 36  Category Two Defects   207 non-null    int64  \n 37  Expiration             207 non-null    object \n 38  Certification Body     207 non-null    object \n 39  Certification Address  207 non-null    object \n 40  Certification Contact  207 non-null    object \ndtypes: float64(13), int64(6), object(22)\nmemory usage: 66.4+ KB\n```\n:::\n:::\n\n\n## Step 3: Clean data\nA lot of these columns are not related to the coffee bean itself, so I'll want to remove those. It also appears that I will need to handle null values, even after I take out the ICO number. I'll go ahead and pare the dataset down to just the columns that are related to the coffee beans, but before I can remove null values I need to get the Altitude values from a string type to an int type so that I can use them more accurately.\n I wrote a function that uses regex to pull out the int values from the Altitude string into an array of stringified ints, which I turn into ints and then average so I only have one value for each Altitude.\n After that, I need the categorical columns to be useable by the machine learning algorithms, so I use a label encoder to turn the categorical data into numerical data. This dataset also isn't very big, so I use an imputer to change null values to plausible values instead of dropping rows with null values.\n\n::: {.cell execution_count=3}\n``` {.python .cell-code}\n# clean data\ncoffee_df_small = coffee_df[['Country of Origin', 'Altitude', 'Region', 'Variety', 'Processing Method', 'Aroma', 'Flavor']].copy()\n\n# altitude has some values that are ranges, which is not ideal because it makes it a string not an int, so we'll take the average and replace the value with that\ndef get_altitude_int(alt_in):\n  ints = re.findall(r'\\b\\d+\\b', alt_in)\n  int_values = [int(num) for num in ints]\n  return int(np.mean(int_values))\n\naltitude_no_na = coffee_df_small['Altitude'].notnull()\ncoffee_df_small.loc[altitude_no_na, 'Altitude Avg'] = coffee_df_small.loc[altitude_no_na, 'Altitude'].apply(get_altitude_int)\n\n# turn categorical columns into numerical\ncoffee_df_nums = coffee_df_small.select_dtypes(include=['number'])\nlabel_encoder = LabelEncoder()\n\nfor col in coffee_df_small.columns:\n    if coffee_df_small[col].dtype == 'object':  # Check if column has categorical data\n        coffee_df_nums[col] = label_encoder.fit_transform(coffee_df_small[col])\n\nimputer = SimpleImputer(strategy=\"median\")\nimputer.fit(coffee_df_nums)\ncoffee_imp = pd.DataFrame(imputer.transform(coffee_df_nums), columns=coffee_df_nums.columns)\nprint(coffee_imp.info())\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 207 entries, 0 to 206\nData columns (total 8 columns):\n #   Column             Non-Null Count  Dtype  \n---  ------             --------------  -----  \n 0   Aroma              207 non-null    float64\n 1   Flavor             207 non-null    float64\n 2   Altitude Avg       207 non-null    float64\n 3   Country of Origin  207 non-null    float64\n 4   Altitude           207 non-null    float64\n 5   Region             207 non-null    float64\n 6   Variety            207 non-null    float64\n 7   Processing Method  207 non-null    float64\ndtypes: float64(8)\nmemory usage: 13.1 KB\nNone\n```\n:::\n:::\n\n\n## Step 4: Cleaning is done, time to investigate feature importance\nI am going to use a random forest classifier to determine what features are most relevant to predicting coffee variety. \n\n::: {.cell execution_count=4}\n``` {.python .cell-code}\n# do random forest to find out what the most important features are\ncoffee_enc_copy = coffee_imp.copy()\ntrain_set_X, test_set_X, train_set_y, test_set_y = train_test_split(coffee_enc_copy.drop('Variety', axis='columns'), coffee_enc_copy['Variety'], test_size=0.2, random_state=42)\n\nrnd_clf_coffee = RandomForestClassifier(n_estimators=500, max_leaf_nodes=16,\n                                 n_jobs=-1, random_state=42)\nrnd_clf_coffee.fit(train_set_X, train_set_y)\n\n# look at the relationship\nfeature_importances = pd.Series(rnd_clf_coffee.feature_importances_, train_set_X.columns).sort_values()\n\nplt.barh(feature_importances.index, feature_importances.values * 100, color='brown')\nplt.ylabel('Feature')\nplt.xlabel('Percentage Feature Importance')\nplt.title('Feature Importance of Coffee Features to Coffee Variety')\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-5-output-1.png){width=693 height=449}\n:::\n:::\n\n\nThere are a couple interesting things to note in this bar chart. The first is the order of the important features: altitude average is most important at a little over 20%, then Country of Origin at around 20%. The second is that altitude average is more important than altitude, but I created altitude average from altitude, so shouldn't they be the same? The reason they are not is that the label encoder assumes there is no relation between the unique values in the column, so a super high altitude could be labeled 0 and a super low altitude could be labeled 207 (since that is the number of items in each column). Clearly, being able to understand that altitude is a numerical value is important to this model. With that in mind, that also means that country of origin might be less useable after being run through a label encoder than if I had found a way of imparting relative geographic location into the numerical representation of country of origin.\n\n## Step 5: Run a K-Means on the top 2 most important coffee features to variety\nThe top 2 features are altitude average and country of origin, so naturally we will do our K-Means clustering algorithm on those variables. I want altitude on the y axis because it is the vertical column and is a good representation of actual altitude. I will also use 7 clusters because that is how many varieties of coffee there are in this dataset.\n\n::: {.cell execution_count=5}\n``` {.python .cell-code}\n# aroma_alt_np = np.array(coffee_imp[['Aroma', 'Altitude Avg']])\naltitude_country_np = np.array(coffee_imp[['Country of Origin', 'Altitude Avg']])\n# k-means clusterer\nk = 7\nkmeans = KMeans(n_clusters=k, random_state=42)\ny_pred = kmeans.fit_predict(altitude_country_np)\n\n# plot decision boundaries\nmins = altitude_country_np.min(axis=0) - 0.1\nmaxs = altitude_country_np.max(axis=0) + 0.1\nxx, yy = np.meshgrid(np.linspace(mins[0], maxs[0], 1000),\n                      np.linspace(mins[1], maxs[1], 1000))\nZ = kmeans.predict(np.c_[xx.ravel(), yy.ravel()])\nZ = Z.reshape(xx.shape)\n\nplt.contourf(Z, extent=(mins[0], maxs[0], mins[1], maxs[1]),\n            cmap=\"copper\", alpha=0.6)\nplt.contour(Z, extent=(mins[0], maxs[0], mins[1], maxs[1]),\n            linewidths=1, colors='k')\n\n# plot data\nsns.scatterplot(data=coffee_imp, x='Country of Origin', y='Altitude Avg', s=20, hue='Variety', palette=sns.color_palette(\"Set2\", as_cmap=True))\n\n# plot centroids\ncentroids = kmeans.cluster_centers_\nplt.scatter(centroids[:, 0], centroids[:, 1],\n                marker='o', s=35, linewidths=8,\n                color='white', zorder=10, alpha=0.9)\nplt.scatter(centroids[:, 0], centroids[:, 1],\n                marker='x', s=2, linewidths=12,\n                color='black', zorder=11, alpha=1)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nC:\\Users\\Erica\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n  super()._check_params_vs_input(X, default_n_init=10)\n```\n:::\n\n::: {.cell-output .cell-output-display execution_count=5}\n```\n<matplotlib.collections.PathCollection at 0x1fb83bf7740>\n```\n:::\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-6-output-3.png){width=601 height=429}\n:::\n:::\n\n\n## Step 6: Reevaluate\nThat chart looks... not great. One reason could be because Country of Origin was originally categorical, and the label encoder may have not done a great job. Just for fun, we'll take the feature with the highest feature importance to variety that started out as numerical - aroma. That is how people can tell between coffee beans, lets hope the same goes for a K-Means clustering algorithm!\n\n::: {.cell execution_count=6}\n``` {.python .cell-code}\naroma_alt_np = np.array(coffee_imp[['Aroma', 'Altitude Avg']])\n# k-means clusterer\nk = 7\nkmeans = KMeans(n_clusters=k, random_state=42)\ny_pred = kmeans.fit_predict(aroma_alt_np)\n\n# plot decision boundaries\nmins = aroma_alt_np.min(axis=0) - 0.1\nmaxs = aroma_alt_np.max(axis=0) + 0.1\nxx, yy = np.meshgrid(np.linspace(mins[0], maxs[0], 1000),\n                      np.linspace(mins[1], maxs[1], 1000))\nZ = kmeans.predict(np.c_[xx.ravel(), yy.ravel()])\nZ = Z.reshape(xx.shape)\n\nplt.contourf(Z, extent=(mins[0], maxs[0], mins[1], maxs[1]),\n            cmap=\"copper\", alpha=0.6)\nplt.contour(Z, extent=(mins[0], maxs[0], mins[1], maxs[1]),\n            linewidths=1, colors='k')\n\n# plot data\nsns.scatterplot(data=coffee_imp, x='Aroma', y='Altitude Avg', s=20, hue='Variety', palette=sns.color_palette(\"Set2\", as_cmap=True))\n\n# plot centroids\ncentroids = kmeans.cluster_centers_\nplt.scatter(centroids[:, 0], centroids[:, 1],\n                marker='o', s=35, linewidths=8,\n                color='white', zorder=10, alpha=0.9)\nplt.scatter(centroids[:, 0], centroids[:, 1],\n                marker='x', s=2, linewidths=12,\n                color='black', zorder=11, alpha=1)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nC:\\Users\\Erica\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n  super()._check_params_vs_input(X, default_n_init=10)\n```\n:::\n\n::: {.cell-output .cell-output-display execution_count=6}\n```\n<matplotlib.collections.PathCollection at 0x1fb869405f0>\n```\n:::\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-7-output-3.png){width=601 height=429}\n:::\n:::\n\n\nThat does not look much better. Looks like we will need a better dataset to be able to do this, or to tweak the encoder enough that the values are more impactful.\n\n",
    "supporting": [
      "index_files"
    ],
    "filters": [],
    "includes": {}
  }
}